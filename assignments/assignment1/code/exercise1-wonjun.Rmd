---
title: "Exercise 1"
author: "Wonjun"
date: "Due: Friday, September 23"
output: 
  bookdown::html_document2:
    number_sections: FALSE
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup.R}
source('setup.R')
WAU(name="wonjun")
```

```{r data.R}
source('data.R')
```

# Questions

The related variables are:`tot_pat_rev`, `uncomp_care` where
`uncomp_care = tot_uncomp_care_charges - tot_uncomp_care_partial_pmts + bad_debt`
from 2011

```{r table_sumstat_rev.R}
source('table_sumstat_rev.R')
```

```{r table_sumstat_uncompcare.R}
source('table_sumstat_uncompcare.R')
```

2.  Create a figure showing the mean hospital uncompensated care from
    2000 to 2018. Show this trend separately by hospital ownership type
    (private not for profit and private for profit).

```{r}
source('fig_uncompcare.R')
```

3.  Using a simple DD identification strategy, estimate the effect of
    Medicaid expansion on hospital uncompensated care using a
    traditional two-way fixed effects (TWFE) estimation:
    \begin{equation}
    y_{it} = \alpha_{i} + \gamma_{t} + \delta D_{it} + \varepsilon_{it},
    (\#eq:dd)
    \end{equation} where $D_{it}=1(E_{i}\leq t)$ in Equation
    \@ref(eq:dd) is an indicator set to 1 when a hospital is in a state
    that expanded as of year $t$ or earlier, $\gamma_{t}$ denotes time
    fixed effects, $\alpha_{i}$ denotes hospital fixed effects, and
    $y_{it}$ denotes the hospital's amount of uncompensated care in year
    $t$. Present four estimates from this estimation in a table: one
    based on the full sample (regardless of treatment timing); one when
    limiting to the 2014 treatment group (with never treated as the
    control group); one when limiting to the 2015 treatment group (with
    never treated as the control group); and one when limiting to the
    2016 treatment group (with never treated as the control group).
    Briefly explain any differences.
    
```{r table_DD_twfe.R}
df_temp <- df %>%
  filter(!is.na(uncomp_care)) %>%
  mutate(Y = uncomp_care/1000000,
         D = ifelse(year>= year_adopted,1,0),
         D = ifelse(!is.na(D),D,0))
reg_DD_twfe <- feols(Y~D| provider + year, data=df_temp)

reg_DD_subsample <- function(ssyr) {
  reg <- feols(Y~D | provider+year,
               data=df_temp %>% 
                 filter(expanded==FALSE |
                        year_adopted==ssyr))
  return(reg)
}
reg_DD_twfe_subsample = lapply(2014:2016, reg_DD_subsample)

twfe <- list(reg_DD_twfe)
twfe <- append(twfe, reg_DD_twfe_subsample)

tab <- msummary(twfe,stars=TRUE,
         coef_rename = c("D"="expand"),
         gof_map=c("nobs", "r.squared"),
         notes="Each column shows the estimation results of (1) Full sample, (2) Only 2014, (3) Only 2015, (4) Only 2016",
         output = "gt")
gt::gtsave(tab, filename=here('output','tab_twfe.tex'))
```

4.  Estimate an "event study" version of the specification in part 3:
    \begin{equation}
    y_{it} = \alpha_{i} + \gamma_{t} +\sum_{\tau < -1} D_{it}^{\tau} \delta_{\tau} + \sum_{\tau>=0} D_{it}^{\tau} \delta_{\tau} + \varepsilon_{it},
    (\#eq:event)
    \end{equation} where $D_{it}^{\tau} = 1(t-E_{i}=\tau)$ in Equation
    \@ref(eq:event) is essentially an interaction between the treatment
    dummy and a relative time dummy. In this notation and context,
    $\tau$ denotes years relative to Medicaid expansion, so that
    $\tau=-1$ denotes the year before a state expanded Medicaid,
    $\tau=0$ denotes the year of expansion, etc. Estimate with two
    different samples: one based on the full sample and one based only
    on those that expanded in 2014 (with never treated as the control
    group).

```{r table_event_study.R}
df_temp <- df %>%
  filter(!is.na(uncomp_care)) %>%
  mutate(uncomp_care = uncomp_care/1000000,
         relative_time = year - as.integer(year_adopted),
         relative_time = ifelse(!is.na(relative_time),relative_time,0))

reg_event <- 
  feols(uncomp_care~i(relative_time, expanded, ref=-1) |provider+year,
        cluster = ~state, data = df_temp)

df_temp <- df_temp %>% filter((expanded==F)|year_adopted==2014)

reg_event_ss <- 
  feols(uncomp_care~i(relative_time,expanded,ref=-1)|provider+year,
        cluster = ~state, data = df_temp)

etable(reg_event, reg_event_ss, tex=T, style.tex=style.tex('aer'),
         file=here("output","tab_event.tex"), replace=T)
```

```{r}
iplot(reg_event)
```



5.  Sun and Abraham(SA) show that the $\delta_{\tau}$ coefficients in
    Equation \@ref(eq:event) can be written as a non-conxvex average of
    all other group-time specific average treatment effects. They
    propose an interaction weighted specification: \begin{equation}
    y_{it} = \alpha_{i} + \gamma_{t} +\sum_{e} \sum_{\tau \neq -1} \left(D_{it}^{\tau} \times 1(E_{i}=e)\right) \delta_{e, \tau} + \varepsilon_{it}.
    (\#eq:iwevent)
    \end{equation} Re-estimate your event study using the SA
    specification in Equation \@ref(eq:iwevent). Show your results for
    $\hat{\delta}_{e, \tau}$ in a Table, focusing on states with
    $E_{i}=2014$, $E_{i}=2015$, and $E_{i}=2016$.

```{r table_sunab.R}
df_temp <- df %>%
  filter(!is.na(uncomp_care)) %>%
  mutate(uncomp_care = uncomp_care/1000000,
         expand_year = ifelse(expanded==FALSE,999,as.integer(year_adopted)),
         time_to_treat = ifelse(expanded==FALSE,-1, year-expand_year))

reg_sunab <- 
  feols(uncomp_care~sunab(expand_year, time_to_treat, no_agg=T)|provider+ year,
        cluster=~state,
        data=df_temp)

tab14 <- esttable(reg_sunab,
                  keep = c("time_to_treat = -\\d+ x cohort = 2014",
                           "time_to_treat = \\d+ x cohort = 2014"))
tab15 <- esttable(reg_sunab,
                  keep = c("time_to_treat = -\\d+ x cohort = 2015",
                           "time_to_treat = \\d+ x cohort = 2015"))
tab16 <- esttable(reg_sunab,
                  keep = c("time_to_treat = -\\d+ x cohort = 2016",
                           "time_to_treat = \\d+ x cohort = 2016"))
tab_list <- list(tab14,tab15,tab16)
f <- function(tab) {
  newtab <- tab
  rnames <- rownames(tab)
  rownames(newtab) <- str_replace(rnames, " x cohort = \\d+","")
  newtab$key <- rownames(newtab)
  newtab <- as.tibble(newtab)
  return(newtab)
}
newtab_list <- lapply(tab_list, f)

tab14 <- f(tab14)
tab15 <- f(tab15)
tab16 <- f(tab16)

jaja <- tab14 %>% 
  left_join(tab15, by='key', suffix=c("14","15")) %>%
  left_join(tab16, by="key", suffix=c("","16"))

rnames <- jaja$key
jaja <- jaja %>% select(!key) %>%
  rename("E = 14" = reg_sunab14,
         "E = 15" = reg_sunab15,
         "E = 16" = reg_sunab)
jaja$" " <- rnames
jaja <- jaja %>% select(" ","E = 14","E = 15","E = 16")
  
print(xtable(jaja),
       file=here('output','tab_sunab.tex'),
       include.rownames=F)

```

6.  Present an event study graph based on the results in part 5. Hint:
    you can do this automatically in `R` with the `fixest` package
    (using the `sunab` syntax for interactions), or with
    `eventstudyinteract` in `Stata`. These packages help to avoid
    mistakes compared to doing the tables/figures manually and also help
    to get the standard errors correct.

```{r fig_sunab.R}
df_temp <- df %>%
  filter(!is.na(uncomp_care)) %>%
  mutate(uncomp_care = uncomp_care/1000000,
         expand_year = ifelse(expanded==FALSE,999,as.integer(year_adopted)),
         time_to_treat = ifelse(expanded==FALSE,-1, year-expand_year))

reg_sunab <- feols(uncomp_care~sunab(expand_year, time_to_treat)|provider+ year,
                  cluster=~state,
                  data=df_temp)

jpeg(filename=here('output','fig_sunab.jpg'))
iplot(reg_sunab)
dev.off()
```

7.  Callaway and Sant'Anna (CS) offer a non-parametric solution that
    effectively calculates a set of group-time specific differences,
    $ATT(g,t)= E[y_{it}(g) - y_{it}(\infty) | G_{i}=g]$, where $g$
    reflects treatment timing and $t$ denotes time. They show that under
    the standard DD assumptions of parallel trends and no anticipation,
    $ATT(g,t) = E[y_{it} - y_{i, g-1} | G_{i}=g] - E[y_{it} - y_{i,g-1} | G_{i} = \infty]$,
    so that $\hat{ATT}(g,t)$ is directly estimable from sample analogs.
    CS also propose aggregations of $\hat{ATT}(g,t)$ to form an overall
    ATT or a time-specific ATT (e.g., ATTs for $\tau$ periods
    before/after treatment). With this framework in mind, provide an
    alternative event study using the CS estimator. Hint: check out the
    `did` package in `R` or the `csdid` package in `Stata`.

```{r tab_CS_event.R}
df_temp <- df %>%
  filter(!is.na(uncomp_care)) %>%
  mutate(uncomp_care = uncomp_care/1000000,
         expand_year = ifelse(expanded==FALSE,0,as.integer(year_adopted))) %>%
  group_by(provider) %>%
  mutate(providergroup=cur_group_id()) %>% ungroup()

reg_CS <- att_gt(yname="uncomp_care",
                 tname="year",
                 idname="providergroup",
                 gname="expand_year",
                 data=df_temp, panel=TRUE, est_method="dr",
                 allow_unbalanced_panel=TRUE)

reg_CS_event <- aggte(reg_CS, type="dynamic")

msummary(reg_CS_event, output=here('output','tab_cs_event.tex'))
```

```{r fig_CS_event.R}
df_temp <- df %>%
  filter(!is.na(uncomp_care)) %>%
  mutate(uncomp_care = uncomp_care/1000000,
         expand_year = ifelse(expanded==FALSE,0,as.integer(year_adopted))) %>%
  group_by(provider) %>%
  mutate(providergroup=cur_group_id()) %>% ungroup()

reg_CS <- att_gt(yname="uncomp_care",
                 tname="year",
                 idname="providergroup",
                 gname="expand_year",
                 data=df_temp, panel=TRUE, est_method="dr",
                 allow_unbalanced_panel=TRUE)

reg_CS_event <- aggte(reg_CS, type="dynamic")

ggdid(reg_CS_event)
ggsave(here('output','fig_CS.jpg'))
```


8.  Rambachan and Roth (RR) show that traditional tests of parallel
    pre-trends may be underpowered, and they provide an alternative
    estimator that essentially bounds the treatment effects by the size
    of an assumed violation in parallel trends. One such bound RR
    propose is to limit the post-treatment violation of parallel trends
    to be no worse than some multiple of the pre-treatment violation of
    parallel trends. Assuming linear trends, such a violation is
    reflected by
    $$\Delta(\bar{M}) = \left\{ \delta : \forall t \geq 0, \lvert (\delta_{t+1} - \delta_{t}) - (\delta_{t} - \delta_{t-1}) \rvert \leq \bar{M} \times \max_{s<0} \lvert (\delta_{s+1} - \delta_{s}) - (\delta_{s} - \delta_{s-1}) \rvert \right\}.$$
    Using the `HonestDiD` package in `R` or `Stata`, present a
    sensitivity plot of your CS ATT estimates using
    $\bar{M} = \{0, 0.5, 1, 1.5, 2\}$. Check out the GitHub repo
    [here](https://github.com/pedrohcgs/CS_RR) for some help in
    combining the `HonestDiD` package with CS estimates.

```{r fig_honest.R}
### First, we import the function Pedro Sant'Anna created for 
##   formatting did output for HonestDiD ####

#' @title honest_did
#'
#' @description a function to compute a sensitivity analysis
#'  using the approach of Rambachan and Roth (2021)
#' @param es an event study
honest_did <- function(es, ...) {
  UseMethod("honest_did", es)
}

#' @title honest_did.AGGTEobj
#'
#' @description a function to compute a sensitivity analysis
#'  using the approach of Rambachan and Roth (2021) when
#'  the event study is estimating using the `did` package
#'
#' @param e event time to compute the sensitivity analysis for.
#'  The default value is `e=0` corresponding to the "on impact"
#'  effect of participating in the treatment.
#' @param type Options are "smoothness" (which conducts a
#'  sensitivity analysis allowing for violations of linear trends
#'  in pre-treatment periods) or "relative_magnitude" (which
#'  conducts a sensitivity analysis based on the relative magnitudes
#'  of deviations from parallel trends in pre-treatment periods).
#' @inheritParams HonestDiD::createSensitivityResults
#' @inheritParams HonestDid::createSensitivityResults_relativeMagnitudes
honest_did.AGGTEobj <- function(es,
                                e=0,
                                type=c("smoothness", "relative_magnitude"),
                                method=NULL,
                                bound="deviation from parallel trends",
                                Mvec=NULL,
                                Mbarvec=NULL,
                                monotonicityDirection=NULL,
                                biasDirection=NULL,
                                alpha=0.05,
                                parallel=FALSE,
                                gridPoints=10^3,
                                grid.ub=NA,
                                grid.lb=NA,
                                ...) {
  
  
  type <- type[1]
  
  # make sure that user is passing in an event study
  if (es$type != "dynamic") {
    stop("need to pass in an event study")
  }
  
  # check if used universal base period and warn otherwise
  if (es$DIDparams$base_period != "universal") {
    warning("it is recommended to use a universal base period for honest_did")
  }
  
  # recover influence function for event study estimates
  es_inf_func <- es$inf.function$dynamic.inf.func.e
  
  # recover variance-covariance matrix
  n <- nrow(es_inf_func)
  V <- t(es_inf_func) %*% es_inf_func / (n*n) 
  
  
  nperiods <- nrow(V)
  npre <- sum(1*(es$egt < 0))
  npost <- nperiods - npre
  
  baseVec1 <- basisVector(index=(e+1),size=npost)
  
  orig_ci <- constructOriginalCS(betahat = es$att.egt,
                                 sigma = V, numPrePeriods = npre,
                                 numPostPeriods = npost,
                                 l_vec = baseVec1)
  
  if (type=="relative_magnitude") {
    if (is.null(method)) method <- "C-LF"
    robust_ci <- createSensitivityResults_relativeMagnitudes(betahat = es$att.egt, sigma = V, 
                                                             numPrePeriods = npre, 
                                                             numPostPeriods = npost,
                                                             bound=bound,
                                                             method=method,
                                                             l_vec = baseVec1,
                                                             Mbarvec = Mbarvec,
                                                             monotonicityDirection=monotonicityDirection,
                                                             biasDirection=biasDirection,
                                                             alpha=alpha,
                                                             gridPoints=100,
                                                             grid.lb=-1,
                                                             grid.ub=1,
                                                             parallel=parallel)
    
  } else if (type=="smoothness") {
    robust_ci <- createSensitivityResults(betahat = es$att.egt,
                                          sigma = V, 
                                          numPrePeriods = npre, 
                                          numPostPeriods = npost,
                                          method=method,
                                          l_vec = baseVec1,
                                        monotonicityDirection=monotonicityDirection,
                                          biasDirection=biasDirection,
                                          alpha=alpha,
                                          parallel=parallel,
                                          Mvec=Mvec)
  }
  
  list(robust_ci=robust_ci, orig_ci=orig_ci, type=type)
}

########

# Now, we run the C&S event-study with 'universal' base-period using the did package
# cs_results <- did::att_gt(
#                      yname = "dins",
#                      tname = "year",
#                      idname = "stfips", 
#                      gname = "yexp2", 
#                      data = df,
#                      control_group = "notyettreated")
# 
# #We create an event-study (using 5 periods before and after)
# es <- did::aggte(cs_results, type = "dynamic", 
#             min_e = -5, max_e = 5)

#Run sensitivity analysis for relative magnitudes 
sensitivity_results <-
honest_did.AGGTEobj(reg_CS_event,
                    e =0,
                    type = "relative_magnitude",
                    Mbarvec = seq(from = 0.5, to = 2, by = 0.5))

HonestDiD::createSensitivityPlot_relativeMagnitudes(sensitivity_results$robust_ci,
                                                    sensitivity_results$orig_ci)
ggsave(here('output','fig_honest.jpg'))

```

9.  Discuss your findings and compare estimates from different
    estimators (e.g., are your results sensitive to different
    specifications or estimators? Are your results sensitive to
    violation of parallel trends assumptions?).

10. Reflect on this assignment. What did you find most challenging? What
    did you find most surprising?
